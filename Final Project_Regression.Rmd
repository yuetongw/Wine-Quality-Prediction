---
title: "Wine Quality Prediction (Regression)"
author: "Jun Li, Yuetong Wang"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    number_section: yes
    fig_height: 4
    fig_width: 4
    fig_align: center
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE)
library(stringr)
library(readr)
library(readxl)
library(corrplot)
library(caret)
library(dplyr)
library(Metrics)
library(rpart)
library(rpart.plot)
library(ranger)
library(randomForest)
library(lubridate)
library(tidyverse)
```

# Data Processing and Description

## Load the dataset

```{r}
wine_quality <- read_csv("./wine-quality-white-and-red.csv", show_col_types = FALSE)
```

## Check the missing value

```{r}
wine_quality <- na.omit(wine_quality)
```

## Change the column name

```{r}
colnames(wine_quality) <- c("type", "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar", "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density", "pH", "sulphates", "alcohol", "quality")
```

## Create the dummy variable for type

```{r}
for (i in 1:nrow(wine_quality)) {
  ifelse(wine_quality$type[i] == "white", wine_quality$type_white[i] <- 1, wine_quality$type_white[i] <- 0)
}
```

## Histogram of the response variable

```{r}
library(ggplot2)

ggplot(wine_quality, aes(x = quality)) +
  geom_histogram(binwidth = 1, color = "black", fill = "white") +
  labs(x = "Quality", y = "Count", title = "Histogram of Wine Quality") +
  theme(axis.text=element_text(size=11)) +
  theme_minimal()

hist(wine_quality$quality)
unique(wine_quality$quality)
```

## Correlation between variables

```{r fig.height=5, fig.width=6}
corr_plot <- cor(wine_quality[,2:14])
corrplot(corr_plot, method = "circle", tl.col="black", tl.cex=0.8)
```

```{r}
corrplot(corr_plot, method = "number", number.cex = 0.7, tl.col="black", tl.cex=0.8)
```

## Select the dependent and independent variables

```{r}
Final_wine_quality <- wine_quality[,2:14]
```


## Scale the predictors

```{r}
scaled_wine_quality <- cbind(scale(Final_wine_quality[, -12]), Final_wine_quality[, 12, drop = FALSE])
```

# Use Validation Set to get the test MSE

## Linear Model

### OLS Model using Best Subset Selection

```{r}
library(leaps)
best_subset_lm <- data.frame(seed = integer(), adj_r_squared = numeric(), num_predictors = integer(), predictors = I(list()), mse = numeric())

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  # Split the data into a training set (70% of the data) and a testing set (30% of the data)
  train_index <- sample(nrow(scaled_wine_quality), nrow(scaled_wine_quality) * 0.7)
  train_data <- scaled_wine_quality[train_index, ]
  test_data <- scaled_wine_quality[-train_index, ]
  
  regfit_full <- regsubsets(quality ~ ., data = train_data, nvmax = ncol(scaled_wine_quality) - 1)
  regfit_summary <- summary(regfit_full)
  
  num_predictors <- which.max(regfit_summary$adjr2)
  max_adj_r_squared <- regfit_summary$adjr2[which.max(regfit_summary$adjr2)]
  
  # Get the predictors included in the best model
  best_model_predictors <- regfit_full$x[regfit_summary$which[which.max(regfit_summary$adjr2),]][-1]
  
  # Fit the linear model using the selected predictors on the training data
  best_model_formula <- as.formula(paste("quality ~", paste(best_model_predictors, collapse = " + ")))
  best_lm <- lm(best_model_formula, data = train_data)
  predictions <- predict(best_lm, newdata = test_data)
  mse <- mean((test_data$quality - predictions)^2)
  
  best_subset_lm <- rbind(best_subset_lm, data.frame(seed = i, adj_r_squared = max_adj_r_squared, num_predictors = num_predictors, predictors = I(list(best_model_predictors)), mse = mse))
}
```

```{r}
mean(best_subset_lm$mse)
```

### Lasso Regression Model

```{r}
library(glmnet)

mse_lasso_vec <- vector("numeric", 100)

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  # Split the data into a training set (70% of the data) and a testing set (30% of the data)
  train_index <- sample(nrow(scaled_wine_quality), nrow(scaled_wine_quality) * 0.7)
  train_data <- scaled_wine_quality[train_index, ]
  test_data <- scaled_wine_quality[-train_index, ]
  
  x_train <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = train_data)[,-1]
  y_train <- train_data$quality
  lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = 0.1)
  
  x_test <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = test_data)[,-1]
  y_test <- test_data$quality
  
  test_pred_lasso <- predict(lasso_model, newx = x_test)
  mse_lasso <- mean((y_test - test_pred_lasso)^2)
  
  # Store the result
  mse_lasso_vec[i] <- mse_lasso
}

mse_lasso_vec
```

```{r}
mean(mse_lasso_vec)
```

### Ridge Regression

```{r}
mse_ridge_vec <- vector("numeric", 100)

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  # Split the data into a training set (70% of the data) and a testing set (30% of the data)
  train_index <- sample(nrow(scaled_wine_quality), nrow(scaled_wine_quality) * 0.7)
  train_data <- scaled_wine_quality[train_index, ]
  test_data <- scaled_wine_quality[-train_index, ]
  
  x_train <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = train_data)[,-1]
  y_train <- train_data$quality
  ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = 0.1)
  
  x_test <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = test_data)[,-1]
  y_test <- test_data$quality
  
  test_pred_ridge <- predict(ridge_model, newx = x_test)
  mse_ridge <- mean((y_test - test_pred_ridge)^2)
  
  # Store the MSE 
  mse_ridge_vec[i] <- mse_ridge
}

mse_ridge_vec
```

```{r}
mean(mse_ridge_vec)
```

## Nonparametric Model

### Decision Tree

```{r}
library(rpart)

mse_tree_vec <- vector("numeric", 100)

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  # Split the data into a training set (70% of the data) and a testing set (30% of the data)
  train_index <- sample(nrow(Final_wine_quality), nrow(Final_wine_quality) * 0.7)
  train_data <- Final_wine_quality[train_index, ]
  test_data <- Final_wine_quality[-train_index, ]
  
  tree_model <- rpart(formula = quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = train_data)
  
  test_pred_tree <- predict(tree_model, newdata = test_data)
  mse_tree <- mean((test_pred_tree - test_data$quality)^2)
  
  # Store the MSE
  mse_tree_vec[i] <- mse_tree
}

mse_tree_vec
```

```{r}
mean(mse_tree_vec)
```

### Random Forest

```{r}
library(ranger)

mse_rf_vec <- vector("numeric", 100)

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  # Split the data into a training set (70% of the data) and a testing set (30% of the data)
  train_index <- sample(nrow(Final_wine_quality), nrow(Final_wine_quality) * 0.7)
  train_data <- Final_wine_quality[train_index, ]
  test_data <- Final_wine_quality[-train_index, ]
  
  rf_model <- ranger(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = train_data, num.trees = 500)
  
  test_pred_rf <- predict(rf_model, data = test_data)$predictions
  mse_rf <- mean((test_pred_rf - test_data$quality)^2)
  
  # Store the MSE 
  mse_rf_vec[i] <- mse_rf
}

mse_rf_vec
```

```{r}
mean(mse_rf_vec)
```

# Use Cross-validation to get the MSE

## Linear Model

### OLS Model

```{r}
train_control <- trainControl(method = "cv", number = 10)

mse_values <- numeric(100)

for (i in 1:100) {
  set.seed(i)
  OLS_model <- train(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = scaled_wine_quality, method = "lm", trControl = train_control)
  mse_values[i] <- OLS_model$results$RMSE^2
}

OLS_cv_mse <- mean(mse_values)
OLS_cv_mse
```

### Lasso Regression Model

```{r}
library(glmnet)
set.seed(1)

x <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = scaled_wine_quality)[,-1] 
y <- scaled_wine_quality$quality 

cv_lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)

# Optimal lambda value 
cv_lasso$lambda.min

# Corresponding cross-validated MSE
lasso_cv_mse <- cv_lasso$cvm[cv_lasso$lambda == cv_lasso$lambda.min]
lasso_cv_mse
```

```{r}
plot(cv_lasso, main='Test MSE vs Lambda (Lasso)')
```

### Ridge Regression

```{r}
set.seed(1)
x <- model.matrix(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = scaled_wine_quality)[,-1] 
y <- scaled_wine_quality$quality 

cv_ridge <- cv.glmnet(x, y, alpha = 0, nfolds = 10)

# Optimal lambda value 
cv_ridge$lambda.min

# Corresponding cross-validated MSE
ridge_cv_mse <- cv_ridge$cvm[cv_ridge$lambda == cv_ridge$lambda.min]
ridge_cv_mse
```

```{r}
plot(cv_ridge, main='Test MSE vs Lambda (Ridge)')
```

## Nonparametric Model

### Decision Tree

```{r}
mse_tree_values <- vector("numeric", 100)

# seeds from 1 to 100
for (i in 1:100) {
  set.seed(i)
  
  train_control <- trainControl(method = "cv", number = 10)
  
  tree_model <- train(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = Final_wine_quality, method = "rpart", trControl = train_control)
  
  mse_tree <- min(tree_model$results$RMSE)^2
  
  # Store the MSE 
  mse_tree_values[i] <- mse_tree
}

tree_cv_mse <- mean(mse_tree_values)
tree_cv_mse
```


### Random Forest


```{r}
library(ranger)
set.seed(1)
train_control <- trainControl(method = "cv", number = 10)

rf_model <- train(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type_white, data = Final_wine_quality, method = "ranger", trControl = train_control)

rf_cv_mse <- min(rf_model$results$RMSE)^2
rf_cv_mse 
```

```{r}
rf_model$bestTune
```

